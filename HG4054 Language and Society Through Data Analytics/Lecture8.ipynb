{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries and data"
      ],
      "metadata": {
        "id": "DWCPiL3_vxzk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Tpokj6hmvkSM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "#for scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#for clustering. we will use scipy today\n",
        "from sklearn.cluster import KMeans  #using scikit-learn\n",
        "from scipy.cluster.vq import kmeans, vq  #using scipy\n",
        "\n",
        "\n",
        "data=pd.read_csv('Lecture8.csv',index_col='Country')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display all columns and rows, adjust image size"
      ],
      "metadata": {
        "id": "gm2UrS6L2RvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows',None)\n",
        "pd.set_option('display.max_columns',None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize']=12,6\n",
        "rcParams['figure.dpi']=300"
      ],
      "metadata": {
        "id": "TNJUeBY02W3y"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard scaling of data: turn values into z-scores"
      ],
      "metadata": {
        "id": "nd7MpEpW2cJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit_transform(data)\n",
        "\n",
        "data=pd.DataFrame(scaler.fit_transform(data),columns=data.columns, index=data.index)\n"
      ],
      "metadata": {
        "id": "mQIO1L0i0B62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization with PCA - how likely will we get a good clustering outcome?\n"
      ],
      "metadata": {
        "id": "z3FQDhDg56oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reduce data to 2D with PCA\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "\n",
        "pca = sklearnPCA(n_components=2) #specify no. of components\n",
        "pca.fit_transform(data)\n",
        "\n",
        "#how much variance each component explains\n",
        "pca.explained_variance_ratio_\n",
        "#cumulative variance explained by all components\n",
        "pca.explained_variance_ratio_.cumsum()\n",
        "#information loss\n",
        "1 - pca.explained_variance_ratio_.cumsum()[1]\n",
        "\n",
        "#for convenience, save reduced data to new dataframe\n",
        "reduced_data = pd.DataFrame(pca.fit_transform(data), columns=['Dim_1','Dim_2'], index=data.index)\n",
        "\n",
        "#plot reduced data\n",
        "sns.scatterplot(reduced_data, x='Dim_1', y='Dim_2')\n",
        "plt.title('Countries along 2D (reduced from 4D)')\n",
        "\n",
        "#access linear combinations of principal components\n",
        "pca.components_\n"
      ],
      "metadata": {
        "id": "1ZrcTys558Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try T-SNE method (t-distributed Stochastic Neighbor Embedding)"
      ],
      "metadata": {
        "id": "6pWz7V1L6RP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)  #t-SNE is stochastic and involves randomization, so set random_state for reproducibility\n",
        "X_tsne = tsne.fit_transform(data)\n",
        "\n",
        "reduced_t = pd.DataFrame(tsne.fit_transform(data), columns=['Dim_1','Dim_2'], index=data.index)\n",
        "\n",
        "sns.scatterplot(x='Dim_1', y='Dim_2', data=reduced_t, s=30)\n",
        "plt.title('Countries along 2D (reduced from 4D) [t-SNE]')\n"
      ],
      "metadata": {
        "id": "1EVjQmQ26Xln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try 3D visualization (Reduce data to 3D)"
      ],
      "metadata": {
        "id": "-mPLW2Ym6hDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = sklearnPCA(n_components=3)\n",
        "data_3D = pca.fit_transform(data)\n",
        "\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.view_init(elev=20, azim=10)\n",
        "\n",
        "ax.scatter(data_3D[:,0], data_3D[:,1], data_3D[:,2])\n",
        "\n",
        "ax.set_xlabel('Dim_1')\n",
        "ax.set_ylabel('Dim_2')\n",
        "ax.set_zlabel('Dim_3')\n",
        "ax.set_title('Countries along 3D (reduced from 4D)')\n",
        "\n",
        "\n",
        "#information loss\n",
        "1 - pca.explained_variance_ratio_.cumsum()[2]"
      ],
      "metadata": {
        "id": "CjbIHUqD6jYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find OPTIMAL K (no. of clusters) WITH ELBOW METHOD"
      ],
      "metadata": {
        "id": "3C0mj1sP6mw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#determine optimal number of clusters with the 'elbow method'\n",
        "num_clusters = np.arange(1, 11)\n",
        "distortion_values = []\n",
        "\n",
        "for i in num_clusters:\n",
        "    #Iterate over each k, train model, and calculate distortion\n",
        "    cluster_centers, distortion = kmeans(data, i)\n",
        "    distortion_values.append(distortion)\n",
        "\n",
        "#generate 'elbow plot'\n",
        "plt.plot(num_clusters, distortion_values, '-o')\n",
        "plt.xlabel('number of clusters, k',fontsize=15)\n",
        "plt.ylabel('distortion value',fontsize=15)\n",
        "plt.title('Elbow plot',fontsize=15)\n",
        "plt.xticks(num_clusters, fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#what happens when no. of clusters = sample size?\n",
        "num_clusters = np.arange(1, len(data)+1)\n",
        "distortion_values = []\n",
        "\n",
        "for i in num_clusters:\n",
        "    cluster_centers, distortion = kmeans(data, i)\n",
        "    distortion_values.append(distortion)\n",
        "\n",
        "plt.plot(num_clusters, distortion_values, '-o')\n",
        "plt.xlabel('number of clusters, k',fontsize=15)\n",
        "plt.ylabel('distortion value',fontsize=15)\n",
        "plt.title('Elbow plot',fontsize=15)\n",
        "plt.xticks(num_clusters, fontsize=6, rotation=60)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ei5s2szu6ulx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find OPTIMAL k WITH SILHOUETTE SCORES"
      ],
      "metadata": {
        "id": "KlQN2pkh68JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Same as above, loop through number of clusters, but start with 2 since silhouette scores need at least 2 clusters to be calculated\n",
        "num_clusters = np.arange(2, 11)\n",
        "\n",
        "silhouette_scores = []\n",
        "\n",
        "for i in num_clusters:\n",
        "    # Iterate over each k, train model, and calculate silhouette score\n",
        "    cluster_centers, distortion = kmeans(data, i)\n",
        "    labels = vq(data, cluster_centers)[0]\n",
        "    silhouette_scores.append(silhouette_score(data, labels))\n",
        "\n",
        "#generate silhouette plot\n",
        "plt.plot(num_clusters,  silhouette_scores, '-o')\n",
        "plt.xlabel('number of clusters, k',fontsize=15)\n",
        "plt.ylabel('silhouette scores',fontsize=15)\n",
        "plt.title('Silhouette plot',fontsize=15)\n",
        "plt.xticks(num_clusters, fontsize=15)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ixpICcsM6Uci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate CLUSTER CENTRES and CLUSTER LABELS"
      ],
      "metadata": {
        "id": "yXhTPmzE9Z1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate cluster centers and labels using the optimal number of clusters\n",
        "cluster_centers, distortion = kmeans(data, 3)    #distortion = distortion value for specified k\n",
        "data['cluster'], distances = vq(data, cluster_centers)  #distances = distance of each datapoint to its cluster center\n",
        "\n",
        "\n",
        "#group countries by cluster and view\n",
        "data.groupby('cluster').get_group(0)\n",
        "data.groupby('cluster').get_group(1)\n",
        "data.groupby('cluster').get_group(2)\n",
        "\n",
        "\n",
        "#a loop to do so\n",
        "groups = data.groupby('cluster')\n",
        "for group_name, group_data in groups:\n",
        "    print(f\"Group {group_name}:\")\n",
        "    print(group_data)\n",
        "    print()\n",
        "\n",
        "\n",
        "#verify that cluster centers are the mean feature scores of all its members\n",
        "cluster_centers\n",
        "data.groupby('cluster').mean()\n",
        "\n",
        "\n",
        "#view cluster information in terms of original, non-scaled features\n",
        "data2=pd.read_csv('Lecture8.csv',index_col='Country')  #reload original data and call it data2\n",
        "data2['cluster'] = data['cluster']  #reattach cluster labels to data2\n",
        "data2.groupby('cluster').mean()  #show cluster centers in terms of original features\n"
      ],
      "metadata": {
        "id": "s6wbZiu49ddz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize clustering outcome"
      ],
      "metadata": {
        "id": "Jo4kH3Oj9lKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#also reduce cluster centers to two dimensions. No need to do so if there are only two clusters\n",
        "pca = sklearnPCA(n_components=2)\n",
        "center_2D = pca.fit_transform(cluster_centers).T    #T = transpose the matrix. An important step\n",
        "\n",
        "#generate scatterplot with cluster centres\n",
        "sns.scatterplot(reduced_data, x='Dim_1', y='Dim_2', hue=data['cluster'], palette='bright', s=30)\n",
        "plt.plot(center_2D[0],center_2D[1],'rX',markersize=12)   #'rX' = red cross\n",
        "\n",
        "#include this block of code to annotate each object\n",
        "for i in range(len(reduced_data)):\n",
        "    plt.text(x=reduced_data.Dim_1[i]+0.05, y=reduced_data.Dim_2[i]+0.05, s=reduced_data.index[i], size=8)\n",
        "plt.legend(title='Cluster')\n",
        "plt.xlabel(\"Dim_1\")\n",
        "plt.ylabel(\"Dim_2\")\n",
        "plt.title(\"Annotated clustering outcome\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nhwKL3Wr9oLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize clustering outcome in 3D"
      ],
      "metadata": {
        "id": "R0EsEpSx-Lwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reduce cluster centers to three dimensions\n",
        "pca = sklearnPCA(n_components=3)\n",
        "center_3D = pca.fit_transform(cluster_centers).T\n",
        "data_3D = pca.fit_transform(data)\n",
        "\n",
        "\n",
        "#generate plot with cluster centres\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.view_init(elev=20, azim=10)\n",
        "\n",
        "colors = ['purple', 'orange', 'green']\n",
        "# Plot each cluster in 3D\n",
        "for i, color in enumerate(colors):\n",
        "    # Only select data points that belong to the current cluster\n",
        "    ix = np.where(data['cluster'] == i)\n",
        "    ax.scatter(data_3D[ix, 0], data_3D[ix, 1], data_3D[ix, 2], c=color, label=f'Cluster {i+1}')\n",
        "\n",
        "# Plot cluster centers\n",
        "ax.scatter(center_3D[0], center_3D[1], center_3D[2], c='red', marker='x', s=150, label='Centroids')\n",
        "\n",
        "ax.legend()\n",
        "ax.set_xlabel('Dim_1')\n",
        "ax.set_ylabel('Dim_2')\n",
        "ax.set_zlabel('Dim_3')\n",
        "ax.set_title('3D clustering outcome',fontsize=15)\n"
      ],
      "metadata": {
        "id": "mPUkObnm-OUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate clustering outcome"
      ],
      "metadata": {
        "id": "y_RW0KfJ-cby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate intra-cluster similarity (WCSS)\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "distances = cdist(data.iloc[:,:-1], cluster_centers, 'euclidean')  #calculate pairwise distance between all points and all centers. use iloc to remove cluster labels first!\n",
        "np.sum(np.min(distances, axis=1)**2)  #for each point, pick the minimum distance; i.e. its cluster center. Then square and sum up all points\n",
        "\n",
        "\n",
        "#Calcluate inter-cluster dissimilarity\n",
        "inter_cluster_distances = cdist(cluster_centers, cluster_centers, 'euclidean')  #calculate pairwise distance between all centers\n",
        "np.max(inter_cluster_distances)  #the maximum distance between any two clusters denotes inter-cluster dissimilarity\n",
        "\n",
        "\n",
        "#Visualize cluster centers for distinctiveness\n",
        "data.groupby('cluster').mean().plot(kind='bar', rot=0)\n",
        "plt.legend(loc='best')\n",
        "\n",
        "\n",
        "\n",
        "#OPTIONAL: Confirm with ANOVA / regression if desired\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "\n",
        "mapping = {0: 'A', 1: 'B', 2: 'C'}\n",
        "data['cluster'] = data['cluster'].replace(mapping)\n",
        "\n",
        "model = ols('GDP_pc ~ cluster', data).fit()  #try different features\n",
        "\n",
        "model.summary() #check p-value\n",
        "sm.stats.anova_lm(model)  #alternative ANOVA table\n"
      ],
      "metadata": {
        "id": "C0XVnkUo-eRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEMINAR 8 CODE: Use only if you're REALLY stuck"
      ],
      "metadata": {
        "id": "v-AL7YsM-ovx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Seminar8.csv', index_col='Country')\n",
        "df2 = df.iloc[:, :-1]\n",
        "\n",
        "df2=pd.DataFrame(scaler.fit_transform(df2),columns=df2.columns, index=df2.index)\n",
        "\n",
        "sns.scatterplot(df2, x='Happiness', y='GDP_log')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Find optimal k with elbow method\n",
        "\"\"\"\n",
        "num_clusters = np.arange(1, 11)\n",
        "distortion_values = []\n",
        "\n",
        "for i in num_clusters:\n",
        "    #Iterate over each k, train model, and calculate distortion\n",
        "    cluster_centers, distortion = kmeans(df2, i)\n",
        "    distortion_values.append(distortion)\n",
        "\n",
        "#generate 'elbow plot'\n",
        "plt.plot(num_clusters, distortion_values, '-o')\n",
        "plt.xlabel('number of clusters, k',fontsize=15)\n",
        "plt.ylabel('distortion value',fontsize=15)\n",
        "plt.title('Elbow plot',fontsize=15)\n",
        "plt.xticks(num_clusters, fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Find optimal k with silhouette scores\n",
        "\"\"\"\n",
        "num_clusters = np.arange(2, 11)\n",
        "silhouette_scores = []\n",
        "\n",
        "for i in num_clusters:\n",
        "    # Iterate over each k, train model, and calculate silhouette score\n",
        "    cluster_centers, distortion = kmeans(df2, i)\n",
        "    labels = vq(df2, cluster_centers)[0]\n",
        "    silhouette_scores.append(silhouette_score(df2, labels))\n",
        "\n",
        "#generate silhouette plot\n",
        "plt.plot(num_clusters,  silhouette_scores, '-o')\n",
        "plt.xlabel('number of clusters, k',fontsize=15)\n",
        "plt.ylabel('silhouette scores',fontsize=15)\n",
        "plt.title('Silhouette plot',fontsize=15)\n",
        "plt.xticks(num_clusters, fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Generate cluster centres and cluster labels\n",
        "\"\"\"\n",
        "cluster_centers, distortion = kmeans(df2, 3)    #distortion = distortion value for specified k\n",
        "df2['cluster'], distances = vq(df2, cluster_centers)  #distances = distance of each datapoint to its cluster center\n",
        "\n",
        "sns.scatterplot(df2, x='Happiness', y='GDP_log', hue='cluster')\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Evaluate clustering outcome\n",
        "\"\"\"\n",
        "#Calculate intra-cluster similarity (WCSS)\n",
        "distances = cdist(df2.iloc[:,:-1], cluster_centers, 'euclidean')  #calculate pairwise distance between all points and all centers. use iloc to remove cluster labels first!\n",
        "np.sum(np.min(distances, axis=1)**2)  #for each point, pick the minimum distance; i.e. its cluster center. Then square and sum up all points\n",
        "\n",
        "#Calcluate inter-cluster dissimilarity\n",
        "inter_cluster_distances = cdist(cluster_centers, cluster_centers, 'euclidean')  #calculate pairwise distance between all centers\n",
        "np.max(inter_cluster_distances)  #the maximum distance between any two clusters denotes inter-cluster dissimilarity\n",
        "\n",
        "#Visualize cluster centers for distinctiveness\n",
        "df2.groupby('cluster').mean().plot(kind='bar', rot=0)\n",
        "plt.legend(loc='best')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Train K-NN classifier\n",
        "\"\"\"\n",
        "# Define outcome and predictors\n",
        "y = df['Regime']\n",
        "x = df2[['Happiness','GDP_log', 'cluster']]\n",
        "\n",
        "\"\"\"\n",
        "Find optimal k\n",
        "\"\"\"\n",
        "# Setup arrays to store accuracy values\n",
        "neighbors = np.arange(1, 16)\n",
        "accuracy = np.empty(len(neighbors))\n",
        "\n",
        "# Loop over different values of k, fit model, and compute accuracy\n",
        "for i, k in enumerate(neighbors):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(x,y)\n",
        "    accuracy[i] = knn.score(x, y)\n",
        "\n",
        "# Generate plot\n",
        "plt.title('k-NN: Varying Number of Neighbors')\n",
        "plt.plot(neighbors, accuracy)\n",
        "plt.xticks(neighbors)\n",
        "plt.xlabel('Number of Neighbors')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "# Create a kNN classifier and fit it to data. If n_neigbors is not specified, the default value=5\n",
        "knn = KNeighborsClassifier(n_neighbors = 2)\n",
        "knn.fit(x,y)\n",
        "knn.score(x,y)\n"
      ],
      "metadata": {
        "id": "Xe0pJ45g-tck"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}