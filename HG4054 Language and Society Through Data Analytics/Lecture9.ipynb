{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWCPiL3_vxzk"
      },
      "source": [
        "Importing libraries and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tpokj6hmvkSM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm2UrS6L2RvS"
      },
      "source": [
        "Display all columns and rows, adjust image size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNJUeBY02W3y"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows',None)\n",
        "pd.set_option('display.max_columns',None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize']=12,6\n",
        "rcParams['figure.dpi']=300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd7MpEpW2cJE"
      },
      "source": [
        "Importing text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mQIO1L0i0B62"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Approach 1: prepare the csv file yourself and import into Python\n",
        "\"\"\"\n",
        "data=pd.read_csv('Lecture9.csv')\n",
        "data=pd.read_csv('https://raw.githubusercontent.com/dennistay1981/Resources/refs/heads/main/HG4054%20Language%20and%20Society%20Through%20Data%20Analytics/Lecture9.csv')\n",
        "\n",
        "\"\"\"\n",
        "Approach 2: If using an IDE like Spyder or VSCode\n",
        "Use python to import multiple text files from a directory, and convert to dataframe.\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "#set the path to the folder containing the text files\n",
        "folder_path = \"/Users/dennistay/desktop/Lecture_samples/\" #a mac path\n",
        "folder_path = \"C:/Users/dztay/Desktop/Lecture_samples\"    #a windows path\n",
        "\n",
        "#initialize empty lists to store the file names and contents\n",
        "file_names = []\n",
        "file_contents = []\n",
        "\n",
        "#loop through each file in the folder and read its contents\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.txt'):\n",
        "        with open(os.path.join(folder_path, file_name), 'r', encoding='utf-16') as file:\n",
        "            file_names.append(file_name)\n",
        "            file_lines = file.readlines()\n",
        "            file_text = ''.join(file_lines).replace('\\n', ' ') #replaces empty linebreaks with a space\n",
        "            file_contents.append(file_text)\n",
        "\n",
        "#create a dataframe from the file names and contents\n",
        "corpus = pd.DataFrame({'file_name': file_names, 'text': file_contents})\n",
        "\n",
        "#save to csv if needed\n",
        "corpus.to_csv('docs.csv')\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Approach 3: If using Google Colab, which is a cloud platform, we can't read files directly from the computer.\n",
        "We need to upload them first.\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "\n",
        "# Upload text files using Google Colab interface\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "# Initialize empty lists to store the file names and contents\n",
        "file_names = []\n",
        "file_contents = []\n",
        "\n",
        "# Loop through each uploaded file and read its contents\n",
        "for file_name in uploaded.keys():\n",
        "    if file_name.endswith('.txt'):\n",
        "        file_text = uploaded[file_name].decode('utf-16').replace('\\n', ' ')\n",
        "        file_names.append(file_name)\n",
        "        file_contents.append(file_text)\n",
        "\n",
        "# Create a dataframe from the file names and contents\n",
        "corpus = pd.DataFrame({'file_name': file_names, 'text': file_contents})\n",
        "\n",
        "# Save to csv if needed\n",
        "corpus.to_csv('docs.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3FQDhDg56oo"
      },
      "source": [
        "Text cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "1ZrcTys558Z_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Using Regex and NLTK\n",
        "\"\"\"\n",
        "import re\n",
        "from nltk.stem import *\n",
        "p_stemmer = PorterStemmer()\n",
        "\n",
        "data.replace('Hong Kong','HK',regex=True,inplace=True)\n",
        "data.replace('HongKong','HK',regex=True,inplace=True)\n",
        "data.replace('Hongkong','HK',regex=True,inplace=True)\n",
        "\n",
        "# Remove punctuation, special characters\n",
        "data['special_removed']=data['Headline'].map(lambda x: re.sub(r'\\W', ' ', x))\n",
        "# Remove all single characters (e.g. s left behind after deleting aposthrophe)\n",
        "data['singlechar_removed']=data['special_removed'].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n",
        "# Substitute multiple spaces with single space (after removing single characters, double spaces are created)\n",
        "data['singlechar_removed2']=data['singlechar_removed'].map(lambda x: re.sub(r'\\s+', ' ', x, flags=re.I))\n",
        "# Remove prefixed 'b' (if text string is in bytes format, a character b is appended with the string. This removes it)\n",
        "data['b_removed']=data['singlechar_removed2'].map(lambda x: re.sub(r'^b\\s+', ' ', x, flags=re.I))\n",
        "# Convert the titles to lowercase\n",
        "data['lower_case'] = data['b_removed'].map(lambda x: x.lower())\n",
        "# Remove numbers (but not numbers within words)\n",
        "data['num_removed'] = data['lower_case'].map(lambda x: re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", x))\n",
        "# Stemming to remove morphological affixes from words, leaving only the word stem\n",
        "data['stemmed'] = data['num_removed'].map(lambda x: p_stemmer.stem(x))\n",
        "# Finally, create final cleaned column as 'processed'\n",
        "data['processed']=data['stemmed']\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Using TEXTHERO (https://texthero.org/)\n",
        "\"\"\"\n",
        "pip install texthero\n",
        "import texthero as hero\n",
        "\n",
        "# if you have problems installing, try this\n",
        "pip install \"gensim==4.2.0\"\n",
        "pip install \"texthero==1.0.5\"\n",
        "\n",
        "#import our data again to more easily compare the outcomes\n",
        "data2=pd.read_csv('Lecture9.csv')\n",
        "data2=pd.read_csv('https://raw.githubusercontent.com/dennistay1981/Resources/refs/heads/main/HG4054%20Language%20and%20Society%20Through%20Data%20Analytics/Lecture9.csv')\n",
        "\n",
        "\n",
        "data2['Headline'] = hero.clean(data2['Headline'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pWz7V1L6RP5"
      },
      "source": [
        "TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1EVjQmQ26Xln"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#apply tfidf vectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,1))  #process up to n-grams (contiguous sequence of n words)\n",
        "vectorizer.fit_transform(data['processed'])\n",
        "\n",
        "#see the list of words/features\n",
        "vectorizer.get_feature_names_out()\n",
        "\n",
        "#get document-term matrix. This is a 'dense matrix' because every element (including the many 0) is stored\n",
        "matrix=(vectorizer.fit_transform(data['processed']).toarray())\n",
        "\n",
        "#x documents, y unique words/features\n",
        "matrix.shape\n",
        "\n",
        "#convert matrix to dataframe, with each feature and its corresponding tfidf score\n",
        "df=pd.DataFrame(matrix, columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "#convert to csv if needed\n",
        "df.to_csv('df.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ol_fvk5afzy"
      },
      "source": [
        "Visualizing outcomes by reducing to 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4XFAC2wwaifI"
      },
      "outputs": [],
      "source": [
        "#PCA: reduce matrix to 2D if needed\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "pca = sklearnPCA(n_components=2)\n",
        "pca.fit_transform(matrix)\n",
        "\n",
        "#view the linear combinations\n",
        "pca.components_\n",
        "\n",
        "#attach reduced 2D back to dataframe for future use\n",
        "data[['Dim1','Dim2']]=pca.fit_transform(matrix)\n",
        "\n",
        "sns.scatterplot(data,x='Dim1',y='Dim2', hue='Source')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mPLW2Ym6hDu"
      },
      "source": [
        "WORD EMBEDDING with large pre-trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjbIHUqD6jYt"
      },
      "outputs": [],
      "source": [
        "#Install and import GENSIM\n",
        "!pip install --upgrade gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "#See list of available pre-trained models. Larger ones take longer to download.\n",
        "print(api.info()['models'].keys())\n",
        "\n",
        "# Load Google News model (300 dimensions)\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Load glove-wiki-gigaword-50 (50 dimensions) (https://nlp.stanford.edu/projects/glove/)\n",
        "model = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Demonstrating word embedding features\n",
        "\"\"\"\n",
        "#displaying the vector for a certain word\n",
        "model['dog']\n",
        "model['not_a_word']\n",
        "\n",
        "#vector algebra\n",
        "#finding most similar words by specifying relations\n",
        "model.most_similar(positive=['woman', 'king'], negative=['male'])\n",
        "model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
        "#calculating similarity index between word pairs\n",
        "model.similarity('woman', 'man')\n",
        "model.similarity('woman', 'literature')\n",
        "model.similarity('man', 'literature')\n",
        "model.similarity('woman', 'engineer')\n",
        "model.similarity('man', 'engineer')\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Derive word embeddings for our data\n",
        "\"\"\"\n",
        "nltk.download('punkt')\n",
        "\n",
        "text_column = data['processed']\n",
        "# Convert the text to a list of sentences\n",
        "text_data = []\n",
        "for text in text_column:\n",
        "    sentence_list = nltk.sent_tokenize(text)\n",
        "    text_data.extend(sentence_list)\n",
        "# Preprocess the text data\n",
        "preprocessed_data = []\n",
        "for sentence in text_data:\n",
        "    preprocessed_sentence = [word.lower() for word in sentence.split() if word.isalpha()]\n",
        "    preprocessed_data.append(preprocessed_sentence)\n",
        "\n",
        "\n",
        "# Derive embeddings\n",
        "embedding_data = []\n",
        "for sentence in preprocessed_data:\n",
        "    sentence_embedding = [model.get_vector(word) for word in sentence if word in model.key_to_index]\n",
        "    if sentence_embedding:\n",
        "        embedding_data.append(sum(sentence_embedding) / len(sentence_embedding))\n",
        "    else:\n",
        "        embedding_data.append(None)\n",
        "\n",
        "#shape of embedding data (227 sentences x 50 or 300 dimensions)\n",
        "np.array(embedding_data).shape\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Convert embeddings to a dataframe.\n",
        "Each row of the DataFrame corresponds to a sentence in the preprocessed data, and each column corresponds to a dimension of the Word2Vec embeddings.\n",
        "\"\"\"\n",
        "#Automatically name columns in sequence\n",
        "embedding = pd.DataFrame(embedding_data, columns=['Dim{}'.format(i) for i in range(1, np.array(embedding_data).shape[1]+ 1)])\n",
        "\n",
        "\n",
        "#reduce embedding to 2D with PCA, if needed\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "pca = sklearnPCA(n_components=2)\n",
        "pca.fit_transform(embedding)\n",
        "\n",
        "\n",
        "#attach reduced 2D back to dataframe, for future use\n",
        "data3=pd.read_csv('Lecture9.csv')\n",
        "data3[['Dim1','Dim2']]=pca.fit_transform(embedding)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFUsKuRXfA2X"
      },
      "source": [
        "SEMINAR 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pKdUULpofCJH"
      },
      "outputs": [],
      "source": [
        "pip install python-docx\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from docx import Document\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "\n",
        "\"\"\"\n",
        "First, download all the Seminar9 doc files.\n",
        "Then create a new folder called 'Seminar9' in your Google drive/Colab Notebooks folder\n",
        "Then upload all the Seminar9 doc files into this folder\n",
        "\n",
        "We need to do this because Google Colab is a cloud platform, so we can't read files directly\n",
        "from our computer. If you are using an IDE like Spyder or VSCode, you can read the files directly using the\n",
        "code below.\n",
        "\"\"\"\n",
        "# Mount your Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Replace 'path/to/your/files' with the actual path to your files in Google Drive\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Seminar9'\n",
        "\n",
        "file_names = []\n",
        "file_contents = []\n",
        "\n",
        "# Loop through each file in the directory and read its contents\n",
        "for file_name in os.listdir(file_path):\n",
        "    if file_name.endswith('.docx'):\n",
        "        # Construct the full file path\n",
        "        full_path = os.path.join(file_path, file_name)\n",
        "        # Read the .docx file content\n",
        "        try:\n",
        "            with open(full_path, 'rb') as f:\n",
        "                doc = Document(io.BytesIO(f.read()))  # Using BytesIO\n",
        "                file_names.append(file_name)\n",
        "                file_text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "                file_contents.append(file_text)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_name}: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "If not using Google colab, use the code below to import files directly from your hard drive\n",
        "\"\"\"\n",
        "#set the path to the folder containing the doc files\n",
        "folder_path = \"/Users/dennistay/desktop/Seminar_samples/\" #a mac path\n",
        "folder_path = \"C:/Users/dztay/Desktop/Seminar_samples\"    #a windows path\n",
        "\n",
        "file_names = []\n",
        "file_contents = []\n",
        "# Loop through each file in the folder and read its contents\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.docx'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        doc = Document(file_path)\n",
        "        file_names.append(file_name)\n",
        "        file_text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "        file_contents.append(file_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a DataFrame from the file names and contents\n",
        "corpus = pd.DataFrame({'file_name': file_names, 'text': file_contents})\n",
        "\n",
        "# Display the DataFrame\n",
        "print(corpus)\n",
        "\n",
        "\n",
        "#create a new column 'source' to label whether each transcript is from the CPC or WHO, based on the filename.\n",
        "#If the filename begins with 'PR', it's from CPC\n",
        "corpus['source'] = corpus['file_name'].apply(lambda x:'CMFA' if pd.Series(x).str.contains(r'^PR.*$').any() else 'WHO')\n",
        "\n",
        "\n",
        "#use texthero to clean the text. If texthero does not work for you, skip this step\n",
        "corpus['text'] = hero.clean(corpus['text'])\n",
        "\n",
        "\n",
        "#apply tfidf vectorizer to transcripts\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
        "matrix=(vectorizer.fit_transform(corpus['text']).toarray())\n",
        "\n",
        "\n",
        "#reduce matrix to 2D with PCA\n",
        "pca = sklearnPCA(n_components=2)\n",
        "\n",
        "corpus[['Dim1','Dim2']]=pca.fit_transform(matrix)\n",
        "\n",
        "#visualize the difference between the two sources\n",
        "sns.scatterplot(corpus, x='Dim1', y='Dim2', hue='source')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
