{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWCPiL3_vxzk"
      },
      "source": [
        "Importing libraries and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tpokj6hmvkSM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "data=pd.read_csv('Lecture5.csv')\n",
        "\n",
        "data=pd.read_csv('https://raw.githubusercontent.com/dennistay1981/Resources/refs/heads/main/HG4054%20Language%20and%20Society%20Through%20Data%20Analytics/Lecture5.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BofzHfGBwBkf"
      },
      "source": [
        "Setting display options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0RFBFTpwD7v"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows',None)\n",
        "pd.set_option('display.max_columns',10)\n",
        "pd.set_option('display.width', 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkLqn3oDynya"
      },
      "source": [
        "Visualizing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lDs23ziGwLkB"
      },
      "outputs": [],
      "source": [
        "sns.lmplot(data,y=\"Happiness\",x=\"Life_exp\")\n",
        "\n",
        "sns.stripplot(data,y=\"Happiness\",x=\"Democracy\", color='black')\n",
        "sns.barplot(data,y=\"Happiness\",x=\"Democracy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3QTADXDzW6b"
      },
      "source": [
        "Regression nodels with STATSMODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMBr7eCvzdPu"
      },
      "outputs": [],
      "source": [
        "#Simple regression. The part in green is called the 'formula' of the regression\n",
        "model1 = ols('Happiness ~ Life_exp', data).fit()\n",
        "model1.summary()\n",
        "\n",
        "model2 = ols('Happiness ~ Democracy',data).fit()\n",
        "model2.summary()\n",
        "\n",
        "data.groupby('Democracy')['Happiness'].mean()\n",
        "\n",
        "#Multiple regression\n",
        "sns.lmplot(data,y='Happiness',x='Life_exp', hue='Democracy')\n",
        "\n",
        "#Option 1: Don't model the interaction. Use the + sign.\n",
        "model3 = ols('Happiness ~ Life_exp + Democracy',data).fit()\n",
        "model3.summary()\n",
        "\n",
        "#Option 2: Model the interaction. Use the * sign.\n",
        "model4 = ols('Happiness ~ Life_exp * Democracy',data).fit()\n",
        "model4.summary()\n",
        "\n",
        "\n",
        "#OPTIONAL. Show that the slope of the interaction term is the difference in the life_exp slope between Demo and non-demos\n",
        "non_demo= ols('Happiness ~ Life_exp', data.loc[data['Democracy']==0]).fit()  #fit model1 with non-demos only\n",
        "non_demo.summary()\n",
        "\n",
        "demo= ols('Happiness ~ Life_exp', data.loc[data['Democracy']==1]).fit() #fit model1 with demos only\n",
        "demo.summary()\n",
        "\n",
        "#calculate slope difference, which is the same as the slope of model4\n",
        "demo.params['Life_exp'] - non_demo.params['Life_exp']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKPcv1tCzpSw"
      },
      "source": [
        "In-sample predictions using fitted models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoKK8aIVzY27"
      },
      "outputs": [],
      "source": [
        "#Predicting outcomes using the best model\n",
        "model4.predict(data)\n",
        "data['Predicted_H']=model4.predict(data)\n",
        "data[['Country','Happiness','Predicted_H']]\n",
        "\n",
        "#Evaluate accuracy with visualizations\n",
        "sns.scatterplot(data, y='Predicted_H', x='Happiness')\n",
        "plt.plot([min(data['Happiness']), max(data['Happiness'])], [min(data['Happiness']), max(data['Happiness'])], color='red')\n",
        "\n",
        "#Evaluate accuracy with MAPE (Mean Absolute Percentage Error)\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "mean_absolute_percentage_error(data['Happiness'], data['Predicted_H'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0FN9OhQz5Kq"
      },
      "source": [
        "Optional: Residual diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mDQN8Pfz7n3"
      },
      "outputs": [],
      "source": [
        "#this is simply the observed values - predicted values\n",
        "model4.resid\n",
        "\n",
        "#visualizing mean of residuals\n",
        "sns.lineplot(model4.resid)\n",
        "plt.axhline(0, color='red')\n",
        "\n",
        "#visualizing distribution of residuals\n",
        "sns.histplot(model4.resid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFppSBKGz-gG"
      },
      "source": [
        "Optional: Regression with SCIKIT-LEARN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQIO1L0i0B62"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#Model 1\n",
        "X=data[['Life_exp']] #Define predictors and outcome\n",
        "y=data[['Happiness']]\n",
        "\n",
        "model1a = LinearRegression()\n",
        "model1a.fit(X, y)\n",
        "model1a.coef_  #slope(s) of predictor(s)\n",
        "model1a.intercept_  #intercept\n",
        "\n",
        "\n",
        "#Model 3\n",
        "X=data[['Life_exp','Democracy']]  #Define predictors and outcome\n",
        "y=data[['Happiness']]\n",
        "\n",
        "model3a = LinearRegression()\n",
        "model3a.fit(X, y)\n",
        "model3a.coef_  #slope(s) of predictor(s)\n",
        "model3a.intercept_  #intercept\n",
        "\n",
        "\n",
        "#Model 4\n",
        "data['X1X2']= data['Life_exp'] * data['Democracy']  #Need to create the interaction term first\n",
        "\n",
        "X=data[['Life_exp','Democracy','X1X2']]\n",
        "y=data[['Happiness']]\n",
        "\n",
        "model4a = LinearRegression()\n",
        "model4a.fit(X, y)\n",
        "model4a.coef_  #slope(s) of predictor(s)\n",
        "model4a.intercept_  #intercept"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9hV8dADAV4T"
      },
      "source": [
        "SEMINAR 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs01ubkQAXRt"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv('Seminar5.csv')\n",
        "\n",
        "data=pd.read_csv('https://raw.githubusercontent.com/dennistay1981/Resources/refs/heads/main/HG4054%20Language%20and%20Society%20Through%20Data%20Analytics/Seminar5.csv')\n",
        "\n",
        "\n",
        "sns.scatterplot(data, y='Test', x='TV', hue='Age')\n",
        "\n",
        "model1 = ols('Test ~ TV', data).fit()\n",
        "model1.summary()\n",
        "\n",
        "\n",
        "model2 = ols('Test ~ Age', data).fit()\n",
        "model2.summary()\n",
        "\n",
        "#multivariate model has higher R2\n",
        "#slope coefficient of TV changes from + to -. Controlling for age, the negative influence of TV\n",
        "#is revealed. This is an example of Simpson's paradox where an overall pattern in a sample is reversed in a sub-group\n",
        "model3 = ols('Test ~ TV + Age', data).fit()\n",
        "model3.summary()\n",
        "\n",
        "#interaction effect is not significant\n",
        "model4 = ols('Test ~ TV * Age', data).fit()\n",
        "model4.summary()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
