{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTd3BzFFcq1/HYX3gE12T3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dennistay1981/Resources/blob/main/Code%20and%20data%20in%20publications/Chapter%3A%20Data%20science%20approaches%20to%20metaphor%20and%20mental%20health/Data_science_approaches_to_mental_health.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descriptive analytics: cross-tabulating metaphor sources and targets"
      ],
      "metadata": {
        "id": "t2i6rHdXN2MT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ooXfiuNxoQ"
      },
      "outputs": [],
      "source": [
        "#Import Python libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "#Import data\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/dennistay1981/Resources/refs/heads/main/Code%20and%20data%20in%20publications/Chapter%3A%20Data%20science%20approaches%20to%20metaphor%20and%20mental%20health/Descriptive.csv')\n",
        "\n",
        "# Cross-tabulation\n",
        "crosstab = pd.crosstab(data['SOURCE'], data['TARGET'])\n",
        "\n",
        "# Observed frequencies\n",
        "observed = crosstab.values\n",
        "\n",
        "# Expected frequencies\n",
        "chi2, p, dof, expected = chi2_contingency(observed)\n",
        "\n",
        "# Pearson's residuals\n",
        "residuals = (observed - expected) / np.sqrt(expected)\n",
        "\n",
        "# Cramer's V\n",
        "n = observed.sum()\n",
        "phi2 = chi2 / n\n",
        "r, k = observed.shape\n",
        "phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
        "rcorr = r - ((r - 1)**2) / (n - 1)\n",
        "kcorr = k - ((k - 1)**2) / (n - 1)\n",
        "cramer_v = np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
        "\n",
        "# Heatmap with observed frequencies\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(crosstab, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
        "plt.title('Cross-tabulation of Metaphor Sources and Targets')\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Source')\n",
        "plt.show()\n",
        "\n",
        "# Heatmap with Pearson's residuals\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(residuals, annot=True, fmt=\".2f\", cmap='Reds', cbar=True)\n",
        "plt.title('Pearson Residuals')\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Source')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Chi-square statistic: {chi2:.2f}\")\n",
        "print(f\"P-value: {p:.3f}\")\n",
        "print(f\"Degrees of freedom: {dof}\")\n",
        "print(f\"Cramer's V: {cramer_v:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diagnostic (ARM)\n"
      ],
      "metadata": {
        "id": "zANJjRLCsDNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Python libraries\n",
        "import pandas as pd\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import association_rules, apriori\n",
        "\n",
        "#Import data\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/dennistay1981/Resources/refs/heads/main/Code%20and%20data%20in%20publications/Chapter%3A%20Data%20science%20approaches%20to%20metaphor%20and%20mental%20health/Diagnostic.csv')\n",
        "\n",
        "#Split ASD and non-ASD individuals into two dataframes\n",
        "data_ASD = data.loc[data['ASD']=='Y']\n",
        "data_NASD = data.loc[data['ASD']=='N']\n",
        "\n",
        "#convert transactions into lists: one for ASD, one for Non-ASD, and one for the overall dataset\n",
        "transactions_ASD = data_ASD['sources'].apply(lambda t: t.split(','))\n",
        "transactions_NASD = data_NASD['sources'].apply(lambda t: t.split(','))\n",
        "transactions_all = data['sources'].apply(lambda t: t.split(','))\n",
        "\n",
        "# Instantiate transaction encoder and identify unique items in transactions\n",
        "encoder = TransactionEncoder().fit(transactions_ASD)\n",
        "encoder2 = TransactionEncoder().fit(transactions_NASD)\n",
        "encoder3 = TransactionEncoder().fit(transactions_all)\n",
        "\n",
        "\n",
        "# One-hot encode transactions\n",
        "onehot_ASD = encoder.transform(transactions_ASD)\n",
        "onehot_NASD = encoder2.transform(transactions_NASD)\n",
        "onehot_all = encoder3.transform(transactions_all)\n",
        "\n",
        "# Convert one-hot encoded data to DataFrame\n",
        "onehot_ASD = pd.DataFrame(onehot_ASD, columns = encoder.columns_)\n",
        "onehot_NASD = pd.DataFrame(onehot_NASD, columns = encoder2.columns_)\n",
        "onehot_all = pd.DataFrame(onehot_all, columns = encoder3.columns_)\n",
        "\n",
        "\n",
        "# Compute frequent itemsets using the Apriori algorithm\n",
        "# i.e. what are the items (max length 3) that (jointly) appear in at least 5% of transactions\n",
        "frequent_itemsets_ASD = apriori(onehot_ASD, min_support = 0.05, max_len = 3, use_colnames = True)  #minimum support value, maximum itemset length to be retained\n",
        "frequent_itemsets_NASD = apriori(onehot_NASD, min_support = 0.05, max_len = 3, use_colnames = True)\n",
        "frequent_itemsets_all = apriori(onehot_all, min_support = 0.05, max_len = 3, use_colnames = True)\n",
        "\n",
        "# Compute all association rules for frequent_itemsets, limiting only to rules with support > 0.05\n",
        "# i.e. both antecedent and consequent items jointly appear in at least 5% of transactions\n",
        "rules_ASD = association_rules(frequent_itemsets_ASD, metric = \"support\", min_threshold = 0.05)\n",
        "rules_NASD = association_rules(frequent_itemsets_NASD, metric = \"support\", min_threshold = 0.05)\n",
        "rules_all = association_rules(frequent_itemsets_all, metric = \"support\", min_threshold = 0.05)\n",
        "\n",
        "# Replace frozen sets with strings\n",
        "rules_ASD['antecedents'] = rules_ASD['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_ASD['consequents'] = rules_ASD['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_NASD['antecedents'] = rules_NASD['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_NASD['consequents'] = rules_NASD['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_all['antecedents'] = rules_all['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_all['consequents'] = rules_all['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Sort rules by confidence, then lift, then support in descending order\n",
        "rules_ASD = rules_ASD.sort_values(['confidence', 'lift', 'support'], ascending=[False, False, False])\n",
        "rules_NASD = rules_NASD.sort_values(['confidence', 'lift', 'support'], ascending=[False, False, False])\n",
        "rules_all = rules_all.sort_values(['confidence', 'lift', 'support'], ascending=[False, False, False])\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_1ylhK3AsIAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}