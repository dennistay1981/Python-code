{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3W3a+71e9sh67DLapEOdn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dennistay1981/Resources/blob/main/Lecture10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries and data"
      ],
      "metadata": {
        "id": "PO407BLaEby4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9FlBLOIEPlu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "data=pd.read_csv('Lecture10.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display all columns and rows, adjust image size"
      ],
      "metadata": {
        "id": "eGbdzM5DEr3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pylab import rcParams\n",
        "rcParams['figure.figsize']=8,4\n",
        "rcParams['figure.dpi']=100\n",
        "\n",
        "pd.set_option('display.max_rows',None)\n",
        "pd.set_option('display.max_columns',None)\n",
        "pd.set_option('display.width', 1000)\n"
      ],
      "metadata": {
        "id": "HDG6Y9kEErQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore dataset with(out) source\n",
        "\n",
        "\n",
        "*   scatterplot\n",
        "*   most frequent words\n",
        "\n"
      ],
      "metadata": {
        "id": "cUVw6vkSFIFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=data,x='Dim1',y='Dim2')\n",
        "plt.title('Scatterplot of headlines (without source info)')\n",
        "\n",
        "\n",
        "sns.scatterplot(data=data,x='Dim1',y='Dim2',hue='Source')\n",
        "plt.title('Scatterplot of headlines by source')\n",
        "\n",
        "\n",
        "\n",
        "#USING TEXTHERO: N most frequent words overall\n",
        "import texthero as hero\n",
        "\n",
        "N = 10\n",
        "hero.top_words(data['Headline'])[:N]\n",
        "\n",
        "#save list to dataframe and create a bar plot\n",
        "topwords = pd.DataFrame(hero.top_words(data['Headline'])[:N])\n",
        "\n",
        "sns.barplot(topwords, x=topwords.index, y='Headline')\n",
        "plt.title('Top 10 words overall')\n",
        "plt.xticks(rotation=60)\n",
        "\n",
        "#most frequent word in SCMP\n",
        "hero.top_words(data.loc[(data.Source =='SCMP')]['Headline'])[:N]\n",
        "scmp_top = pd.DataFrame(hero.top_words(data.loc[(data.Source =='SCMP')]['Headline'])[:N])\n",
        "\n",
        "sns.barplot(data=scmp_top, x=scmp_top.index, y='Headline')\n",
        "plt.title('Top 10 words (SCMP)')\n",
        "plt.xticks(rotation=60)\n",
        "\n",
        "\n",
        "#most frequent word in CD\n",
        "hero.top_words(data.loc[(data.Source =='CD')]['Headline'])[:N]\n",
        "CD_top = pd.DataFrame(hero.top_words(data.loc[(data.Source =='CD')]['Headline'])[:N])\n",
        "\n",
        "sns.barplot(data=CD_top, x=CD_top.index, y='Headline')\n",
        "plt.title('Top 10 words (CD)')\n",
        "plt.xticks(rotation=60)\n",
        "\n",
        "\n",
        "#most frequent word in HKFP\n",
        "hero.top_words(data.loc[(data.Source =='HKFP')]['Headline'])[:N]\n",
        "HKFP_top = pd.DataFrame(hero.top_words(data.loc[(data.Source =='HKFP')]['Headline'])[:N])\n",
        "\n",
        "sns.barplot(data=HKFP_top, x=HKFP_top.index, y='Headline')\n",
        "plt.title('Top 10 words (HKFP)')\n",
        "plt.xticks(rotation=60)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#USING SCIKIT-LEARN: top words by tfidf-scores.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
        "array= vectorizer.fit_transform(data['Headline']).toarray()\n",
        "# Sum the TF-IDF values for each feature across all documents\n",
        "total_tfidf_scores = array.sum(axis=0)\n",
        "# Get the feature names\n",
        "feature_names =vectorizer.get_feature_names_out()\n",
        "# Create a dictionary of feature names and their total TF-IDF scores\n",
        "features_dict = dict(zip(feature_names, total_tfidf_scores))\n",
        "# Sort the features by their total TF-IDF score in descending order\n",
        "sorted_features = sorted(features_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "# Print the top 10 most frequent features\n",
        "for feature, score in sorted_features[:10]:\n",
        "    print(f\"{feature}: {score}\")\n",
        "\n",
        "\n",
        "#plot top words by scores\n",
        "topwords = pd.DataFrame(sorted_features[:10], columns=['word','score'])\n",
        "sns.barplot(topwords, x='word',y='score')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qOVp1tR5FTIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification"
      ],
      "metadata": {
        "id": "QnexnlXoF1XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Define y and x\n",
        "y = data['Source']\n",
        "x = data.iloc[:,5:]  #D1 to D300 are from column 5 onwards\n",
        "\n",
        "# Perform train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "# Find best k\n",
        "neighbors = np.arange(1, 16)\n",
        "train_accuracy = np.empty(len(neighbors))\n",
        "test_accuracy = np.empty(len(neighbors))\n",
        "average_accuracy = np.empty(len(neighbors))\n",
        "\n",
        "# Loop over different values of k, fit model, and compute accuracy\n",
        "for i, k in enumerate(neighbors):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(x_train,y_train)\n",
        "    train_accuracy[i] = knn.score(x_train,y_train)\n",
        "    test_accuracy[i] = knn.score(x_test,y_test)\n",
        "    average_accuracy[i] = (train_accuracy[i] + test_accuracy[i]) / 2\n",
        "\n",
        "# Generate plot\n",
        "plt.title('k-NN: Varying Number of Neighbors')\n",
        "plt.plot(neighbors, train_accuracy, label='train accuracy')\n",
        "plt.plot(neighbors, test_accuracy, label='test accuracy')\n",
        "plt.plot(neighbors, average_accuracy, label='avg accuracy')\n",
        "plt.xticks(neighbors)\n",
        "plt.xlabel('Number of Neighbors')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Fit the classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=2)\n",
        "knn.fit(x,y)\n",
        "\n",
        "# Evaluate accuracy\n",
        "knn.score(x, y)\n",
        "\n",
        "# Confusion matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(data['Source'], knn.predict(x))\n",
        "cnf_matrix\n",
        "\n",
        "labels = data['Source'].unique()\n",
        "\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"Blues\", yticklabels=labels, xticklabels=labels, annot_kws={\"size\": 25})\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n"
      ],
      "metadata": {
        "id": "5QystfFVF2_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification report"
      ],
      "metadata": {
        "id": "GgZxMbN5F8MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y, knn.predict(x)))  #actual labels, followed by predicted labels"
      ],
      "metadata": {
        "id": "o-AJNfMeGADX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble learning"
      ],
      "metadata": {
        "id": "8cG877bvGDPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use these three classifiers\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Instantiate them.\n",
        "# Classifiers have optimal parameters that should also be independently determined, to optimize the ensemble.\n",
        "# But we are skipping this step.\n",
        "knn = KNeighborsClassifier()\n",
        "lr = LogisticRegression()\n",
        "svc = SVC()\n",
        "\n",
        "\n",
        "# Decision trees and Naive bayes are another two common classifiers. We leave them out for now\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb = MultinomialNB()\n",
        "\n",
        "\n",
        "# Define our list of three classifiers.\n",
        "classifiers = [('K Nearest Neighbours',knn), ('Logistic Regression',lr), ('SVC',svc)]\n",
        "\n",
        "# Iterate over the pre-defined list of classifiers, and evaluate predictions\n",
        "for clf_name, clf in classifiers:\n",
        "    clf.fit(x, y)\n",
        "    print(clf_name,':', clf.score(x,y))\n",
        "\n",
        "\n",
        "# Use a VOTING CLASSIFIER to determine final result\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "# Instantiate voting classifier\n",
        "vc = VotingClassifier(estimators=classifiers)\n",
        "vc.fit(x, y)\n",
        "\n",
        "print('Voting Classifier accuracy:', vc.score(x,y))\n",
        "\n",
        "\n",
        "\n",
        "# Confusion matrix\n",
        "cnf_matrix = metrics.confusion_matrix(y, vc.predict(x))\n",
        "\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"Blues\", yticklabels=labels, xticklabels=labels, annot_kws={\"size\": 25})\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "\n",
        "# Classification report\n",
        "print(metrics.classification_report(y, vc.predict(x)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FonIPCPkGGnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEMINAR 10"
      ],
      "metadata": {
        "id": "V5ossx5LGJ5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('Seminar10.csv')\n",
        "data.head()  #examine the first 5 rows\n",
        "\n",
        "# 50000 rows may be too much to process on older computers with lower memory\n",
        "# we therefore randomly sample just 10% of the data, and make sure we STRATIFY this sample by sentiment category\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the DataFrame into training and testing sets, stratified by the label column\n",
        "# Remember to standardize random_state so everyone will have the same sample\n",
        "data_90, data = train_test_split(data, test_size=0.1, stratify=data['sentiment'], random_state=42)\n",
        "\n",
        "# check the new data size and split of sentiments\n",
        "len(data)\n",
        "data.groupby('sentiment').count()\n",
        "#reset the index for better organization\n",
        "data=data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "TEXT CLEANING\n",
        "\"\"\"\n",
        "import re\n",
        "from nltk.stem import *\n",
        "p_stemmer = PorterStemmer()\n",
        "\n",
        "# Remove punctuation, special characters\n",
        "data['special_removed']=data['review'].map(lambda x: re.sub(r'\\W', ' ', x))\n",
        "# Remove all single characters (e.g. s left behind after deleting aposthrophe)\n",
        "data['singlechar_removed']=data['special_removed'].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n",
        "# Substitute multiple spaces with single space (after removing single characters, double spaces are created)\n",
        "data['singlechar_removed2']=data['singlechar_removed'].map(lambda x: re.sub(r'\\s+', ' ', x, flags=re.I))\n",
        "# Remove prefixed 'b' (if text string is in bytes format, a character b is appended with the string. This removes it)\n",
        "data['b_removed']=data['singlechar_removed2'].map(lambda x: re.sub(r'^b\\s+', ' ', x, flags=re.I))\n",
        "# Convert the titles to lowercase\n",
        "data['lower_case'] = data['b_removed'].map(lambda x: x.lower())\n",
        "# Remove numbers (but not numbers within words)\n",
        "data['num_removed'] = data['lower_case'].map(lambda x: re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", x))\n",
        "# Stemming to remove morphological affixes from words, leaving only the word stem\n",
        "data['stemmed'] = data['num_removed'].map(lambda x: p_stemmer.stem(x))\n",
        "# Finally, create final cleaned column as 'processed'\n",
        "data['processed']=data['stemmed']\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "TF-IDF VECTORIZATION\n",
        "\"\"\"\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#apply tfidf vectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,1))  #process up to n-grams (contiguous sequence of n words)\n",
        "vectorizer.fit_transform(data['processed'])\n",
        "\n",
        "#convert document-term matrix (sparse matrix) to dense matrix (similar to data['tfidf'] above)\n",
        "array=(vectorizer.fit_transform(data['processed']).toarray())\n",
        "\n",
        "\n",
        "#reduce array to 2D for visualization\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "pca = sklearnPCA(n_components=2)\n",
        "data[['Dim1','Dim2']]=pca.fit_transform(array)  #attach 2D back to dataset\n",
        "\n",
        "#plot scatterplot\n",
        "sns.scatterplot(data=data,x='Dim1',y='Dim2',hue='sentiment')\n",
        "\n",
        "\n",
        "# Define outcome label and predictors. Remember we are using the uncompressed tfidf array.\n",
        "# You can try x=data['Dim1','Dim2'] if you want\n",
        "y = data['sentiment']\n",
        "x = array\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ENSEMBLE LEARNING\n",
        "\"\"\"\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        "# Try default parameters for classifiers\n",
        "knn = KNeighborsClassifier()\n",
        "lr = LogisticRegression()\n",
        "nb = MultinomialNB()\n",
        "\n",
        "# Define our list of three classifiers.\n",
        "classifiers = [('K Nearest Neighbours',knn), ('Logistic Regression',lr), ('Naive Bayes', nb)]\n",
        "\n",
        "# Iterate over the pre-defined list of classifiers, and evaluate predictions\n",
        "for clf_name, clf in classifiers:\n",
        "    clf.fit(x, y)\n",
        "    print(clf_name,':', clf.score(x,y))\n",
        "\n",
        "\n",
        "# Use a VOTING CLASSIFIER to determine final result\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "# Instantiate voting classifier\n",
        "vc = VotingClassifier(estimators=classifiers)\n",
        "vc.fit(x, y)\n",
        "\n",
        "print('Voting Classifier accuracy:', vc.score(x,y))\n",
        "\n",
        "\n",
        "\n",
        "# Confusion matrix\n",
        "cnf_matrix = metrics.confusion_matrix(data['sentiment'], vc.predict(x))\n",
        "cnf_matrix\n",
        "\n",
        "labels = data.groupby('sentiment').count().index\n",
        "\n",
        "# Heatmap. Setting fmt=\".0f\" will display the full number instead of scientific notation\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"Blues\", yticklabels=labels ,xticklabels= labels, annot_kws={\"size\": 25},fmt=\".0f\")\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "\n",
        "# Classification Report\n",
        "print(metrics.classification_report(y, vc.predict(x)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Try refitting the voting classifier model with your own review and predicting its sentiment!\n",
        "\"\"\"\n",
        "\n",
        "# use loc to add your 'review' to the end of the dataframe\n",
        "data.loc[len(data),'processed'] = \"do not spend a single cent of your hard earned money on this pathetic excuse for a so called movie\"\n",
        "\n",
        "# check that it's there\n",
        "data['processed']\n",
        "\n",
        "\n",
        "# what about this 'review'?\n",
        "data.loc[len(data),'processed'] = \"the most brilliant thing I have ever seen, everything was choreographed to perfection, what a fantastic treat\"\n",
        "\n",
        "new_array=(vectorizer.fit_transform(data['processed']).toarray())\n",
        "vc.predict(new_array)[-1]\n",
        "\n",
        "\n",
        "# recalculate the tfidf matrix, and predict the label of the final row (i.e. your review!)\n",
        "new_array=(vectorizer.fit_transform(data['processed']).toarray())\n",
        "vc.predict(new_array)[-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "tmDeXUIoGLf3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}